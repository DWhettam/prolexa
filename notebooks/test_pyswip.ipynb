{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning pyswip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.version\n",
    "#from pyswip import Prolog\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sys.path.append(\"../python\")\n",
    "from POS_tagger import Tagger\n",
    "\n",
    "#pl = Prolog()\n",
    "\n",
    "PROLEXA_PATH = \"../prolog/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports (consults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.consult(PROLEXA_PATH + \"prolexa.pl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-25 17:27:21,622 loading file /home/panjh/.flair/models/en-pos-ontonotes-v0.5.pt\n",
      "TEST\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pyswip import Variable\n",
    "\n",
    "def isplural(word):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemma = wnl.lemmatize(word, 'n')\n",
    "    plural = True if word is not lemma else False\n",
    "    return plural\n",
    "\n",
    "def handle_utterance_str(text) :\n",
    "    if text[0] != \"'\" and text[0] != '\"' :\n",
    "        text = f'\"{text}\"'\n",
    "    \n",
    "    text = text.replace('\"', '\\\"')\n",
    "    text = text.replace(\"'\", '\\\"')\n",
    "    \n",
    "    return \"handle_utterance(1,{},Output)\".format(text)\n",
    "\n",
    "def tag(text) :\n",
    "    tagged_sent, sent, tags = Tagger().tag(text)\n",
    "    return tags\n",
    "\n",
    "def update_rules(text):\n",
    "    tags = tag(text)\n",
    "    text = text.split(' ')\n",
    "    start = ''\n",
    "    end = ''\n",
    "    f = open(PROLEXA_PATH + \"prolexa_grammar.pl\", \"r\")\n",
    "    lines=f.readlines()\n",
    "    f.close()\n",
    "    myiter = iter(lines)\n",
    "    for idx, line in enumerate(myiter):\n",
    "        if not text:\n",
    "            break\n",
    "        if ('DT' in tags) and re.match(r\"determiner\\([a-z],X=>B,X=>H,\\[\\(H:-B\\)\\]\\)(.*)\", line):\n",
    "            start = '--> ['\n",
    "            end = ']'\n",
    "            exists = False\n",
    "            input_word = text[tags.index('DT')]\n",
    "            for det_idx, det_line in enumerate(lines[idx:]):                                                \n",
    "                if not(re.match(r\"determiner\\([a-z],X=>B,X=>H,\\[\\(H:-B\\)\\]\\)(.*)\", det_line)):\n",
    "                    det_idx = det_idx + idx   \n",
    "                    if tags:\n",
    "                        tags.remove('DT')\n",
    "                    if text:\n",
    "                        text.remove(input_word)\n",
    "                    break\n",
    "                line_word = (det_line.split(start))[1].split(end)[0]  \n",
    "                if input_word == line_word:                    \n",
    "                    exists = True\n",
    "                    if tags:\n",
    "                        tags.remove('DT')\n",
    "                    if text:\n",
    "                        text.remove(input_word)\n",
    "                    break \n",
    "\n",
    "            if not exists:\n",
    "                if isplural(input_word):\n",
    "                    new_line = \"determiner(p,X=>B,X=>H,[(H:-B)]) --> [{}].\\n\".format(input_word) \n",
    "                else:\n",
    "                    new_line = \"determiner(s,X=>B,X=>H,[(H:-B)]) --> [{}].\\n\".format(input_word) \n",
    "                lines.insert(det_idx, new_line)\n",
    "                #next(myiter, None)\n",
    "                    \n",
    "        if ('NN' in tags) and re.match(r\"pred\\((.*)[1],\\[(.*)\\]\\)\\.\", line): \n",
    "            start = 'pred('\n",
    "            end = ', '\n",
    "            exists = False\n",
    "            new_line = ''\n",
    "            input_word = text[tags.index('NN')]\n",
    "            for noun_idx, noun_line in enumerate(lines[idx:]):\n",
    "                if not(re.match(r\"pred\\((.*)[1],\\[(.*)\\]\\)\\.\", noun_line)):\n",
    "                    noun_idx = noun_idx + idx   \n",
    "                    if tags:\n",
    "                        tags.remove('NN')\n",
    "                    if text:\n",
    "                        text.remove(input_word)\n",
    "                    break\n",
    "                line_word = (noun_line.split(start))[1].split(end)[0]\n",
    "                if input_word == line_word:\n",
    "                    if (re.match(r\"pred\\((.*)[1](.*)n\\/(.*)\\]\\)\\.\", noun_line)):\n",
    "                        exists = True\n",
    "                        if tags:\n",
    "                            tags.remove('NN')\n",
    "                        if text:\n",
    "                            text.remove(input_word)\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"TEST\")\n",
    "                        noun_idx = noun_idx + idx\n",
    "                        insert_idx = noun_line.index(']).')\n",
    "                        new_line = noun_line[:insert_idx] + ',n/' + input_word + noun_line[insert_idx:]\n",
    "                        lines[noun_idx] = new_line\n",
    "                        exists = True\n",
    "                        if tags:\n",
    "                            tags.remove('NN')\n",
    "                        if text:\n",
    "                            text.remove(input_word)\n",
    "                        break\n",
    "\n",
    "            if not exists:\n",
    "                if new_line == '':\n",
    "                    new_line = 'pred(' + input_word + ', 1,[n/' + input_word + ']).\\n'\n",
    "                lines.insert(noun_idx, new_line)\n",
    "            \n",
    "                    \n",
    "\n",
    "\n",
    "    f = open(PROLEXA_PATH + \"prolexa_grammar.pl\", \"w\")\n",
    "    lines = \"\".join(lines)\n",
    "    f.write(lines)\n",
    "    f.close()\n",
    "            \n",
    "\n",
    "def escape_and_call_prolexa(text) :\n",
    "    libPrefix = \"prolexa:\"\n",
    "    update_rules(text)\n",
    "    return \"\"\n",
    "    #return pl.query(libPrefix + handle_utterance_str(text))\n",
    "\n",
    "\n",
    "out = escape_and_call_prolexa('the human is')\n",
    "list(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-25 17:25:44,053 loading file /home/panjh/.flair/models/en-pos-ontonotes-v0.5.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('every <DT> human <NN> is <VBZ> mortal <JJ>',\n",
       " 'every human is mortal',\n",
       " ['DT', 'NN', 'VBZ', 'JJ'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Tagger().tag(\"I have no idea what I'm doing\")\n",
    "#Tagger().tag(\"Penguins re mrotal\")\n",
    "Tagger().tag(\"every human is mortal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswip import Prolog, Functor, Variable, Query, call\n",
    "\n",
    "pl = Prolog()\n",
    "pl.consult(\"../prolog/prolexa.pl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_utterance = Functor(\"handle_utterance\", 3)\n",
    "Output = Variable()\n",
    "q = Query(handle_utterance(1, \"Peter is mortal.\", Output), )\n",
    "\n",
    "while q.nextSolution() :\n",
    "    print(Output.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.nextSolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.nextSolution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.consult(\"trains.pl\")\n",
    "STATIONS = [ans[\"S\"] for ans in pl.query(\"station(S)\")]\n",
    "STATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.assertz(\"father(michael,john)\")\n",
    "pl.assertz(\"father(michael,gina)\")\n",
    "pl = Prolog()\n",
    "\n",
    "for soln in pl.query(\"father(X,Y)\"):\n",
    "    print(soln[\"X\"], \"is the father of\", soln[\"Y\"])\n",
    "\n",
    "# michael is the father of john\n",
    "# michael is the father of gina\n",
    "\n",
    "childrenOfMichael = pl.query(\"father(michael,X)\")\n",
    "list(childrenOfMichael)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
